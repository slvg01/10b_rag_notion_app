{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "# Access the API token from the configuration file\n",
    "huggingfacehub_api_token = config.get('huggingface', 'api_token')\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = config.get('openai', 'api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #show the natove evaluation criteria\n",
    "from langchain.evaluation import Criteria\n",
    "list(Criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1. Is the submission referring to a real quote from the text?\\n- The quote \"42\" is indeed the answer provided in the book \"The Hitchhiker\\'s Guide to the Galaxy\" by Douglas Adams.\\n- Therefore, the submission meets the criteria.', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "#checking based on native crietria (relevance)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load evaluator, assign it to criteria, and return result\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"42\",\n",
    "    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"- market_potential: The submission does not effectively assess the market potential of the startup. It simply states that investing in a startup focused on flying cars is ridiculous without providing any analysis or justification.\\n- innovation: The submission does not highlight the startup's innovation and uniqueness in its sector. It only dismisses the idea without mentioning any specific innovative aspects of the startup.\\n- risk_assessment: The submission does not provide a thorough analysis of potential risks and mitigation strategies. It simply states that investing in the startup is ridiculous without delving into specific risks or how they could be managed.\\n- scalability: The submission does not address the startup's scalability and growth potential. It solely focuses on rejecting the idea without considering the potential for growth.\", 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "#based on custome criteria \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Add a scalability criterion to custom_criteria\n",
    "custom_criteria = {\n",
    "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
    "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
    "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
    "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
    "}\n",
    "\n",
    "# Criteria an evaluator from custom_criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
    "    prediction=\"No, that is ridiculous.\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='MANIFESTO OF SURREALISM BY ANDRÉ BRETON (1924)\\u2029 \\nMain Author is Andre  Breton', metadata={'source': '../database/MANIFESTO_OF_SURREALISM.pdf', 'page': 0}), Document(page_content='. \\nSo strong is the belief in life, in what is most fragile in life – real life, I mean – that in the end this belief\\nis lost', metadata={'source': '../database/MANIFESTO_OF_SURREALISM.pdf', 'page': 0}), Document(page_content='. Man, that inveterate dreamer, daily more discontent with his destiny, has trouble assessing the\\nobjects he has been led to use, objects that his nonchalance has brought his way, or that he has earned\\nthrough his own efforts, almost always through his own efforts, for he has agreed to work, at least he has\\nnot refused to try his luck (or what he calls his luck!)', metadata={'source': '../database/MANIFESTO_OF_SURREALISM.pdf', 'page': 0}), Document(page_content='. At this point he feels extremely modest: he knows\\nwhat women he has had, what silly affairs he has been involved in; he is unimpressed by his wealth or his\\npoverty, in this respect he is still a newborn babe and, as for the approval of his conscience, I confess that\\nhe does very nicely without it', metadata={'source': '../database/MANIFESTO_OF_SURREALISM.pdf', 'page': 0}), Document(page_content='. If he still retains a certain lucidity, all he can do is turn back toward his\\nchildhood which, however his guides and mentors may have botched it, still strikes him as somehow\\ncharming', metadata={'source': '../database/MANIFESTO_OF_SURREALISM.pdf', 'page': 0})]\n",
      "Question 1: What is the main topic of the document?\n",
      "Expected Answer: surrealism\n",
      "Model Prediction: The main topic of the document is a subject that would require a long and detailed discussion.\n",
      "\n",
      "Question 2: does the document mention a war \n",
      "Expected Answer: yes, a war of ideas\n",
      "Model Prediction:  Yes, the document mentions a war in which the author is participating and is proud to be a part of.\n",
      "\n",
      "Question 3: Who is the Main Author of the document?\n",
      "Expected Answer: Andre Breton\n",
      "Model Prediction: \n",
      "Robert Desnos\n",
      "\n",
      "[{'results': ' INCORRECT'}, {'results': ' CORRECT'}, {'results': ' INCORRECT'}]\n"
     ]
    }
   ],
   "source": [
    "#eval from start doc load\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.evaluation import QAEvalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader('../database/MANIFESTO_OF_SURREALISM.pdf')\n",
    "data = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data)\n",
    "print(docs[:5])\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding)\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
    "\n",
    "\n",
    "#set the questions set base \n",
    "question_set = [{\"question\": \"What is the main topic of the document?\",\"answer\": \"surrealism\"},\n",
    "                {\"question\": \"does the document mention a war \", \"answer\": \"yes, a war of ideas\"    },\n",
    "                {\"question\": \"Who is the Main Author of the document?\",\"answer\": \"Andre Breton\"    }]\n",
    "\n",
    "\n",
    "# Generate the model responses using the RetrievalQA chain and question_set\n",
    "predictions = qa.apply(question_set)\n",
    "\n",
    "# Define the evaluation chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Evaluate the ground truth against the answers that are returned\n",
    "results = eval_chain.evaluate(question_set,\n",
    "                              predictions,\n",
    "                              question_key=\"question\",\n",
    "                              prediction_key=\"result\",\n",
    "                              answer_key='answer')\n",
    "\n",
    "for i, q in enumerate(question_set):\n",
    "    print(f\"Question {i+1}: {q['question']}\")\n",
    "    print(f\"Expected Answer: {q['answer']}\")\n",
    "    print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
